# Understanding Foundation Models

Foundation models are large AI models trained on massive datasets and designed to be adapted to many tasks—such as text, images, coding, or speech—without needing to be retrained from scratch.

## Example
GPT-4, Claude, and Gemini are foundation models.  
You can use them for writing, coding, chatbots, translation, summarization, and more, all without building a model from the ground up.


## Training Data

Training data is the huge collection of information used to teach a foundation model.


## Multilingual Models

Models trained on many languages.

* Example:
A multilingual model trained on English, Spanish, Hindi, Arabic, and Chinese can translate, summarize, or chat across these languages.


## Domain-Specific Models

Models trained for a specific field.

* Example:
A medical model trained on radiology reports can detect diseases better than general models.


## Modeling

How the model is designed.

## Model Architecture

The structure of the neural network.

Example:
Transformers (used in GPT) allow the model to understand context and long sentences better than older neural networks.


## Model Size

How many parameters the model has.

Example:

Small model: 1B parameters → faster, cheaper

Large model: 175B parameters → smarter, better reasoning


## Post-Training

After base training, models gain extra skills.

## Supervised Finetuning

Training with labeled examples.

Example:
Give the model 10,000 examples of “correct answers” so it learns how to help users properly.

## Preference Finetuning

Training with human preferences (what humans like more).

Example:
Two answers are given. Humans choose which is better.
The model learns to respond kindly, safely, and clearly.


## Sampling

How the model generates text.

## Sampling Fundamentals

The model predicts the next token (word piece) by probability.

Example:
If the model predicts:

“cat” → 60%

“dog” → 30%

“cow” → 10%
It usually picks “cat.”


## Sampling Strategies

Different methods for picking words.

Example:

Greedy sampling: always pick the highest probability word

Top-k sampling: pick among the top k words

Temperature: controls creativity


## Test Time Compute

Using extra computation while generating to improve quality.

Example:
The model tries multiple candidate answers and picks the best one. Similar to brainstorming before finalizing.


## The Probabilistic Nature of AI

AI does not give the same answer every time because it predicts using probabilities.

Example:
Ask the model “Write a poem about cats” 5 times → every poem is different.